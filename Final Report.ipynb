{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee634c19-680b-4ca2-8363-eabfd44c0dd8",
   "metadata": {},
   "source": [
    "# STAT301 Final Project - Group 20\n",
    "\n",
    "## Introduction\n",
    "Our global economy drives and depends on functioning companies all around the world, and the characteristics that all organizations have in common are their need and value of their workforce. Employees are one of the most valuable assets and retaining quantity and quality of employees are crucial in running a dependable organization (Negi et. Al, 2013). The failure to do so is called employee attrition, which is a costly challenge faced by many employers around the world. The latter however, is inevitable and managing it is a challenge. Understanding reasons or predictors that could explain employee attrition would provide managers with areas to focus and improve on in the future to improve employee retention (Frye et. Al, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e717a82-5d55-44ea-ac1f-145cd48c9d06",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "The dataset chosen: HR Analytics (Konapure & Uikey, 2023) containing 35 different employee factors and characteristics that can be analyzed to understand and interpret organizational data, detecting possible trends and patterns which can be generalized and implemented by companies to assist attrition management. \n",
    "\n",
    "**Brief Summary of Entire Dataset**\n",
    "\n",
    "|Continuous Data|Nominal Data|Ordinal Data|Binary Data|  \n",
    "|----------------|------------|------------|---------|\n",
    "|Age           | Employee ID | Frequency of Business Travel | Employee Attrition|\n",
    "|Daily Wage | Department | Education Level | Over 18|\n",
    "|Distance to Office | Field of Qualification | Enviromental Satsifcation | Worked Overtime|\n",
    "|Hourly Rate | Gender | Job Involvement |\n",
    "|Monthly Income | Marital Status | Job Level |\n",
    "|Monthly Rate | Job Role | Job Satisfaction \n",
    "|Number of Companies Worked | | Performance Rating \n",
    "|Percentage Salary Hike | | Relationship Satisfaction \n",
    "|Standard Hours | | Stock Option Level \n",
    "|Total Working Years| | Work Life Balance\n",
    "|Training Time |\n",
    "|Years at Company |\n",
    "|Years in Current Role |\n",
    "|Years Since Last Position \n",
    "|Years with Current Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6842504-90a7-4870-ab8d-84dc6d1d9768",
   "metadata": {},
   "source": [
    "## Research Question\n",
    "In our report, we aim to answer the question: **“How can we predict when employees are going to leave?”**\n",
    "\n",
    "“How can we predict when employees are going to leave? (attrition = 1)” (N)\n",
    "* Modifying logistic regression to prefer attrition = 1, mention how there’s an imbalance in the response classes\n",
    "* Discuss how employees departing the company will be a minority (often) of individuals, however we need to design a model using all employee data that can be used to predict the small number of employees that are leaving\n",
    "\n",
    "“How can we understand what variables are contributing to employee attrition?”\n",
    "* Using post-lasso to be able to conduct inference\n",
    "* Idea: splitting data into 3; training-postlasso-test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4933d4-70f7-4194-8020-4049695b07d8",
   "metadata": {},
   "source": [
    "# Section 2a. Exploratory Data Analysis\n",
    "To conduct our analysis, we will need to: load the dataset, clean it, and convert it to a tidy format. Below, we load our dataset from the *HR-Employee-Attrition.csv*, convert character columns to factors, and perform a simple check for NA values. Luckily our dataset is already relatively clean from empty values. However, from the dataset description we know that a number of the numeric columns such as `Education` and `JobLevel` represent categorial data. To address this, we will convert all numeric columns with less than 5 distinct values to factors. Finally, we remove superfluous columns such as `Over18`, `EmployeeCount`, `EmployeeNumber`, and `StandardHours` which will not help us predict MonthlyIncome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bc552e-4be1-4a06-afd1-3e80f7e40639",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(suppressWarnings({\n",
    "  library(ggplot2)\n",
    "  library(knitr)\n",
    "  library(tidyverse)\n",
    "  library(gridExtra)\n",
    "  library(kableExtra)\n",
    "  library(glmnet)\n",
    "  library(dplyr)\n",
    "  library(IRdisplay)\n",
    "  library(webshot)\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269f851d-23d8-43da-9ad1-8e4eafb15b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of NA values: 0\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 31</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Age</th><th scope=col>Attrition</th><th scope=col>BusinessTravel</th><th scope=col>DailyRate</th><th scope=col>Department</th><th scope=col>DistanceFromHome</th><th scope=col>Education</th><th scope=col>EducationField</th><th scope=col>EnvironmentSatisfaction</th><th scope=col>Gender</th><th scope=col>⋯</th><th scope=col>PerformanceRating</th><th scope=col>RelationshipSatisfaction</th><th scope=col>StockOptionLevel</th><th scope=col>TotalWorkingYears</th><th scope=col>TrainingTimesLastYear</th><th scope=col>WorkLifeBalance</th><th scope=col>YearsAtCompany</th><th scope=col>YearsInCurrentRole</th><th scope=col>YearsSinceLastPromotion</th><th scope=col>YearsWithCurrManager</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>⋯</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>41</td><td>Yes</td><td>Travel_Rarely    </td><td>1102</td><td>Sales                 </td><td>1</td><td>2</td><td>Life Sciences</td><td>2</td><td>Female</td><td>⋯</td><td>3</td><td>1</td><td>0</td><td> 8</td><td>0</td><td>1</td><td> 6</td><td>4</td><td>0</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>49</td><td>No </td><td>Travel_Frequently</td><td> 279</td><td>Research &amp; Development</td><td>8</td><td>1</td><td>Life Sciences</td><td>3</td><td><span style=white-space:pre-wrap>Male  </span></td><td>⋯</td><td>4</td><td>4</td><td>1</td><td>10</td><td>3</td><td>3</td><td>10</td><td>7</td><td>1</td><td>7</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>37</td><td>Yes</td><td><span style=white-space:pre-wrap>Travel_Rarely    </span></td><td>1373</td><td>Research &amp; Development</td><td>2</td><td>2</td><td><span style=white-space:pre-wrap>Other        </span></td><td>4</td><td><span style=white-space:pre-wrap>Male  </span></td><td>⋯</td><td>3</td><td>2</td><td>0</td><td> 7</td><td>3</td><td>3</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>33</td><td>No </td><td>Travel_Frequently</td><td>1392</td><td>Research &amp; Development</td><td>3</td><td>4</td><td>Life Sciences</td><td>4</td><td>Female</td><td>⋯</td><td>3</td><td>3</td><td>0</td><td> 8</td><td>3</td><td>3</td><td> 8</td><td>7</td><td>3</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>27</td><td>No </td><td><span style=white-space:pre-wrap>Travel_Rarely    </span></td><td> 591</td><td>Research &amp; Development</td><td>2</td><td>1</td><td><span style=white-space:pre-wrap>Medical      </span></td><td>1</td><td><span style=white-space:pre-wrap>Male  </span></td><td>⋯</td><td>3</td><td>4</td><td>1</td><td> 6</td><td>3</td><td>3</td><td> 2</td><td>2</td><td>2</td><td>2</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>32</td><td>No </td><td>Travel_Frequently</td><td>1005</td><td>Research &amp; Development</td><td>2</td><td>2</td><td>Life Sciences</td><td>4</td><td><span style=white-space:pre-wrap>Male  </span></td><td>⋯</td><td>3</td><td>3</td><td>0</td><td> 8</td><td>2</td><td>2</td><td> 7</td><td>7</td><td>3</td><td>6</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 31\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & Age & Attrition & BusinessTravel & DailyRate & Department & DistanceFromHome & Education & EducationField & EnvironmentSatisfaction & Gender & ⋯ & PerformanceRating & RelationshipSatisfaction & StockOptionLevel & TotalWorkingYears & TrainingTimesLastYear & WorkLifeBalance & YearsAtCompany & YearsInCurrentRole & YearsSinceLastPromotion & YearsWithCurrManager\\\\\n",
       "  & <int> & <fct> & <fct> & <int> & <fct> & <int> & <fct> & <fct> & <fct> & <fct> & ⋯ & <fct> & <fct> & <fct> & <int> & <int> & <fct> & <int> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & 41 & Yes & Travel\\_Rarely     & 1102 & Sales                  & 1 & 2 & Life Sciences & 2 & Female & ⋯ & 3 & 1 & 0 &  8 & 0 & 1 &  6 & 4 & 0 & 5\\\\\n",
       "\t2 & 49 & No  & Travel\\_Frequently &  279 & Research \\& Development & 8 & 1 & Life Sciences & 3 & Male   & ⋯ & 4 & 4 & 1 & 10 & 3 & 3 & 10 & 7 & 1 & 7\\\\\n",
       "\t3 & 37 & Yes & Travel\\_Rarely     & 1373 & Research \\& Development & 2 & 2 & Other         & 4 & Male   & ⋯ & 3 & 2 & 0 &  7 & 3 & 3 &  0 & 0 & 0 & 0\\\\\n",
       "\t4 & 33 & No  & Travel\\_Frequently & 1392 & Research \\& Development & 3 & 4 & Life Sciences & 4 & Female & ⋯ & 3 & 3 & 0 &  8 & 3 & 3 &  8 & 7 & 3 & 0\\\\\n",
       "\t5 & 27 & No  & Travel\\_Rarely     &  591 & Research \\& Development & 2 & 1 & Medical       & 1 & Male   & ⋯ & 3 & 4 & 1 &  6 & 3 & 3 &  2 & 2 & 2 & 2\\\\\n",
       "\t6 & 32 & No  & Travel\\_Frequently & 1005 & Research \\& Development & 2 & 2 & Life Sciences & 4 & Male   & ⋯ & 3 & 3 & 0 &  8 & 2 & 2 &  7 & 7 & 3 & 6\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 31\n",
       "\n",
       "| <!--/--> | Age &lt;int&gt; | Attrition &lt;fct&gt; | BusinessTravel &lt;fct&gt; | DailyRate &lt;int&gt; | Department &lt;fct&gt; | DistanceFromHome &lt;int&gt; | Education &lt;fct&gt; | EducationField &lt;fct&gt; | EnvironmentSatisfaction &lt;fct&gt; | Gender &lt;fct&gt; | ⋯ ⋯ | PerformanceRating &lt;fct&gt; | RelationshipSatisfaction &lt;fct&gt; | StockOptionLevel &lt;fct&gt; | TotalWorkingYears &lt;int&gt; | TrainingTimesLastYear &lt;int&gt; | WorkLifeBalance &lt;fct&gt; | YearsAtCompany &lt;int&gt; | YearsInCurrentRole &lt;int&gt; | YearsSinceLastPromotion &lt;int&gt; | YearsWithCurrManager &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 41 | Yes | Travel_Rarely     | 1102 | Sales                  | 1 | 2 | Life Sciences | 2 | Female | ⋯ | 3 | 1 | 0 |  8 | 0 | 1 |  6 | 4 | 0 | 5 |\n",
       "| 2 | 49 | No  | Travel_Frequently |  279 | Research &amp; Development | 8 | 1 | Life Sciences | 3 | Male   | ⋯ | 4 | 4 | 1 | 10 | 3 | 3 | 10 | 7 | 1 | 7 |\n",
       "| 3 | 37 | Yes | Travel_Rarely     | 1373 | Research &amp; Development | 2 | 2 | Other         | 4 | Male   | ⋯ | 3 | 2 | 0 |  7 | 3 | 3 |  0 | 0 | 0 | 0 |\n",
       "| 4 | 33 | No  | Travel_Frequently | 1392 | Research &amp; Development | 3 | 4 | Life Sciences | 4 | Female | ⋯ | 3 | 3 | 0 |  8 | 3 | 3 |  8 | 7 | 3 | 0 |\n",
       "| 5 | 27 | No  | Travel_Rarely     |  591 | Research &amp; Development | 2 | 1 | Medical       | 1 | Male   | ⋯ | 3 | 4 | 1 |  6 | 3 | 3 |  2 | 2 | 2 | 2 |\n",
       "| 6 | 32 | No  | Travel_Frequently | 1005 | Research &amp; Development | 2 | 2 | Life Sciences | 4 | Male   | ⋯ | 3 | 3 | 0 |  8 | 2 | 2 |  7 | 7 | 3 | 6 |\n",
       "\n"
      ],
      "text/plain": [
       "  Age Attrition BusinessTravel    DailyRate Department            \n",
       "1 41  Yes       Travel_Rarely     1102      Sales                 \n",
       "2 49  No        Travel_Frequently  279      Research & Development\n",
       "3 37  Yes       Travel_Rarely     1373      Research & Development\n",
       "4 33  No        Travel_Frequently 1392      Research & Development\n",
       "5 27  No        Travel_Rarely      591      Research & Development\n",
       "6 32  No        Travel_Frequently 1005      Research & Development\n",
       "  DistanceFromHome Education EducationField EnvironmentSatisfaction Gender ⋯\n",
       "1 1                2         Life Sciences  2                       Female ⋯\n",
       "2 8                1         Life Sciences  3                       Male   ⋯\n",
       "3 2                2         Other          4                       Male   ⋯\n",
       "4 3                4         Life Sciences  4                       Female ⋯\n",
       "5 2                1         Medical        1                       Male   ⋯\n",
       "6 2                2         Life Sciences  4                       Male   ⋯\n",
       "  PerformanceRating RelationshipSatisfaction StockOptionLevel TotalWorkingYears\n",
       "1 3                 1                        0                 8               \n",
       "2 4                 4                        1                10               \n",
       "3 3                 2                        0                 7               \n",
       "4 3                 3                        0                 8               \n",
       "5 3                 4                        1                 6               \n",
       "6 3                 3                        0                 8               \n",
       "  TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole\n",
       "1 0                     1                6             4                 \n",
       "2 3                     3               10             7                 \n",
       "3 3                     3                0             0                 \n",
       "4 3                     3                8             7                 \n",
       "5 3                     3                2             2                 \n",
       "6 2                     2                7             7                 \n",
       "  YearsSinceLastPromotion YearsWithCurrManager\n",
       "1 0                       5                   \n",
       "2 1                       7                   \n",
       "3 0                       0                   \n",
       "4 3                       0                   \n",
       "5 2                       2                   \n",
       "6 3                       6                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main developer: Bill\n",
    "# Contributor: Caden\n",
    "\n",
    "# Load the data and check for NA values\n",
    "hr_data <- read.csv(\"HR-Employee-Attrition.csv\", stringsAsFactors = TRUE)\n",
    "print(paste(\"Number of NA values:\", sum(is.na(hr_data))))\n",
    "\n",
    "# Convert numeric categorial columns to factors\n",
    "hr_data <- hr_data |> \n",
    "    mutate_if(function(col) {\n",
    "        is.numeric(col) & n_distinct(col) <= 5 \n",
    "    }, as.factor)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "hr_data <- hr_data |> \n",
    "    select(-Over18, -EmployeeCount, -EmployeeNumber, -StandardHours)\n",
    "\n",
    "# Extract response vector and data matrix\n",
    "y <- hr_data$Attrition\n",
    "X <- hr_data |> select(-Attrition)\n",
    "\n",
    "# Display data preview\n",
    "head(hr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96041d73-bc27-4afc-a283-6210424ae663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaAAAAQ4CAMAAADIPItgAAADAFBMVEUAAAABAQECAgIDAwME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\nMaCJiCyKAU1EZFEMaCIii2JAExFZ1N/Bga6Oyl1jlwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 540,
       "width": 720
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main developer: Nusaiba\n",
    "# Contributor: Caden\n",
    "options(repr.plot.width = 12, repr.plot.height = 9)\n",
    "barplot <- hr_data |>\n",
    "  ggplot() +\n",
    "  geom_bar(aes(x = Attrition), stat = \"count\", \n",
    "           col = \"grey15\", fill = \"grey65\") +\n",
    "  geom_text(\n",
    "    stat = \"count\",\n",
    "    aes(\n",
    "      x = Attrition, \n",
    "      label = paste0(after_stat(count), \" Employees: \",\n",
    "      100*round(after_stat(count / sum(count)), 2), \n",
    "      \"% of total.\")\n",
    "    ), \n",
    "    vjust = -0.5,\n",
    "    size = 5\n",
    "  ) +\n",
    "  ggtitle(\"Distribution of Attrition across Employees in HR Dataset\") +\n",
    "  xlab(\"Attrition Status\") +\n",
    "  ylab(\"Employee Count\") +\n",
    "  theme_bw() +\n",
    "  theme(\n",
    "    plot.title = element_text(color = \"Black\", size = 20, face = \"bold\"),\n",
    "    axis.title.x = element_text(size = 16),\n",
    "    axis.title.y = element_text(size = 16),\n",
    "    axis.text.x = element_text(size = 14),\n",
    "    axis.text.y = element_text(size = 14)\n",
    "  )\n",
    "\n",
    "barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff182e-f692-4445-8858-8175ee54648e",
   "metadata": {},
   "source": [
    "# Section 2b: Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805c392-f80a-4c4e-89db-f6a36e2687fb",
   "metadata": {},
   "source": [
    "Now we define a new function for assessing the performance of a logistic regression model. Given a Confusion Matrix `conf_in`, it extracts the number of True Positives (`TP`), False Positives (`FP`), True Negatives (`TN`) and False Negatives (`NP`). From these values, we compute Accuracy, Precision, Sensitivity and Specificity and return it as a `list`. In addition, we compute the $F_1$ and $F_{\\beta}$ scores by the following:\n",
    "$$\n",
    "F_{\\beta} = \\frac{(1 + \\beta^2) \\cdot \\text{TP}} {(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}} \\hspace{0.75cm} \\overset{\\beta = 1}{\\longrightarrow} \\hspace{0.75cm} F_{1} = \\frac{2 \\text{TP}} {2 \\text{TP} + \\text{FN} + \\text{FP}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576d0fac-5ddc-46fe-b1a9-b04bf2415417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main developer: Caden\n",
    "# compute the performance metrics given a conf. matrix\n",
    "compute_metrics <- function(conf_in, beta = 2){\n",
    "  # extract table info\n",
    "  TP <- conf_in[2, 2]\n",
    "  FP <- conf_in[2, 1]\n",
    "  TN <- conf_in[1, 1]\n",
    "  FN <- conf_in[1, 2]\n",
    "  # compute metrics\n",
    "  accuracy <- (TP + TN) / sum(conf_in)\n",
    "  precision <- TP / (TP + FP)\n",
    "  sensitivity <- TP / (TP + FN)\n",
    "  specificity <- TN / (TN + FP)\n",
    "  # compute F1 score\n",
    "  F1 <-  2*TP / (2*TP + FP + FN)\n",
    "  # compute F-beta score. Defaults to F2\n",
    "  FB <- (1 + beta^2) * TP / ((1 + beta^2) * TP + beta^2 * FN + FP)\n",
    "  # return everything\n",
    "  return(list(\n",
    "    ACC = accuracy,\n",
    "    PRE = precision,\n",
    "    SEN = sensitivity,\n",
    "    SPC = specificity,\n",
    "    F1  = F1,\n",
    "    FB  = FB\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee912490-0ec1-4978-8fa6-6e4be0559ad9",
   "metadata": {},
   "source": [
    "Now, we fix the seed and designate a training set and an inference set. We will use a $60-40$ split. We also define the `model.matrix` to get our covariates ready for input into `glmnet`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffba77d8-cce9-4857-ac03-ca9f9917b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main developer: Caden\n",
    "# Contributor: Bill\n",
    "set.seed(42)\n",
    "# rename to match my conventions\n",
    "df <- hr_data\n",
    "# train-test split: 60-40\n",
    "train_indices <- sample(1:nrow(df), size = 0.60 * nrow(df), replace = FALSE)\n",
    "train <- df[train_indices, ]\n",
    "# get unselected indices\n",
    "inference  <- df[-train_indices, ]\n",
    "# prepare data for glmnet\n",
    "x <- model.matrix(Attrition ~ . - 1, data = train)\n",
    "y <- as.numeric(train$Attrition == \"Yes\")\n",
    "# write train and inference response as boolean\n",
    "train$Attrition <- as.numeric(train$Attrition == \"Yes\")\n",
    "inference$Attrition <- as.numeric(inference$Attrition == \"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccc4ad-24f0-4630-94db-c3af048a658d",
   "metadata": {},
   "source": [
    "Then, for weights $w \\in [1,10]$ we apply a weight of $w$ to the set of $\\{y \\in \\mathbf{\\vec{y}} \\mid y = 1\\}$ to follow the procedure described in the Methods section. Then, for each weight, we construct a model $\\mathcal{M}_w$ by selecting the variables using Lasso Constraints and cross-validation. Then, we compute and record the in-sample metrics and store the model information in the `models_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d79e2f2-1218-459f-bde8-95014bc496ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and lists\n",
    "weights_list <- 1:10\n",
    "models_list <- list()\n",
    "results_rw <- list()\n",
    "perf_list <- list()\n",
    "# iterate across weights results\n",
    "for(w in weights_list){\n",
    "  \n",
    "  # increase weight of employee leaving by `w`\n",
    "  weights <- ifelse(y == 1, w, 1)\n",
    "  \n",
    "  # ----------------------- #\n",
    "  # --- Logistic LASSO ---- #\n",
    "  # ----------------------- #\n",
    "  # CV Lasso Logistic\n",
    "  cv_logistic <- cv.glmnet(\n",
    "    x, y,\n",
    "    # using LASSO \n",
    "    alpha = 1,\n",
    "    # weights in this iteration\n",
    "    weights = weights,\n",
    "    # logistic regression\n",
    "    family = \"binomial\",\n",
    "    # 15-fold CV\n",
    "    nfolds = 15\n",
    "  )\n",
    "  # in-sample predictions\n",
    "  yhat_lasso <- as.numeric(predict(\n",
    "    cv_logistic,\n",
    "    newx = x,\n",
    "    s = \"lambda.min\",\n",
    "    type = \"response\"\n",
    "  ) > 0.5)\n",
    "  # compute confusion matrix metrics\n",
    "  mat_ins_w <- compute_metrics(table(yhat_lasso, y), beta = 2)\n",
    "  \n",
    "  # ----------------------- #\n",
    "  # --- Selected Vars. ---- #\n",
    "  # ----------------------- #\n",
    "  # then extract chosen variables\n",
    "  coefs_min <- coef(cv_logistic, s = \"lambda.min\")\n",
    "  s_min <- rownames(coefs_min)[which(coefs_min != 0)]\n",
    "  s_min <- s_min[s_min != \"(Intercept)\"]\n",
    "  # match training columns to the original columns\n",
    "  matched_columns <- unique(sapply(s_min, function(var) {\n",
    "    # match selected columns+factor levels to\n",
    "    matches <- colnames(df)[which(sapply(colnames(df), function(orig)\n",
    "      grepl(orig, var)))]\n",
    "    if (length(matches) > 0) {\n",
    "      # get longest match name (e.g. EducationField vs. Education)\n",
    "      matches[which.max(nchar(matches))]\n",
    "    }\n",
    "  }))\n",
    "  # store chosen variables\n",
    "  models_list[[w]] <- matched_columns\n",
    "\n",
    "  # ---------------------- #\n",
    "  # --- Perf. Metrics ---- #\n",
    "  # ---------------------- #\n",
    "  # report metrics\n",
    "  results_rw[[w]] <- data.frame(train = round(\n",
    "    c(\n",
    "      ACC  = mat_ins_w$ACC,\n",
    "      PRE  = mat_ins_w$PRE,\n",
    "      SEN  = mat_ins_w$SEN,\n",
    "      SPC  = mat_ins_w$SPC,\n",
    "      # also extract the F1 score\n",
    "      F1  = mat_ins_w$F1,   \n",
    "      # and the F2 (beta = 2) score\n",
    "      FB  = mat_ins_w$FB,\n",
    "      # estimated difference in accuracy-sensitivity\n",
    "      DLT  = abs(mat_ins_w$ACC - mat_ins_w$SEN),\n",
    "      # estimated difference in accuracy-sensitivity\n",
    "      AVG  = mean(mat_ins_w$ACC, mat_ins_w$SEN),\n",
    "      # also report Mean Cross-Validated Error at Lambda Min\n",
    "      CVM  = cv_logistic$cvm[which(cv_logistic$lambda == cv_logistic$lambda.min)],\n",
    "      # and the Estimate of Standard Error of CVM\n",
    "      CVSE = cv_logistic$cvsd[which(cv_logistic$lambda == cv_logistic$lambda.min)]\n",
    "    ),\n",
    "    3\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cf0d4-86be-4c44-be98-50e92a22d5db",
   "metadata": {},
   "source": [
    "Then, we clean the results of the algorithm and compute $w^{\\star}$ as $\\text{argmin}_{w \\in [1, \\omega]}(\\mathcal{P}_w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8798d42e-f24e-482a-9c36-f3d18eebf0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight corresponding to the best training-set LASSO under the F1 statistic is w=2.\n",
      "The weight corresponding to the best training-set LASSO under the FB statistic is w=6."
     ]
    }
   ],
   "source": [
    "# Main developer: Caden\n",
    "# --------------------- #\n",
    "# --- Clean Output ---- #\n",
    "# --------------------- #\n",
    "results_clean <- do.call(cbind, results_rw)\n",
    "colnames(results_clean) <- paste0(\"w=\", weights_list)\n",
    "# which has the highest F1 score\n",
    "best_w_f1 <- which.max(results_clean[\"F1\", ])\n",
    "# which has the highest FB score\n",
    "best_w_fb <- which.max(results_clean[\"FB\", ])\n",
    "cat(paste0(\"The weight corresponding to the best training-set LASSO under the F1 statistic is w=\",  best_w_f1, \".\\n\"))\n",
    "cat(paste0(\"The weight corresponding to the best training-set LASSO under the FB statistic is w=\", best_w_fb, \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31decea-b44f-41b9-bd73-cc9c345819e8",
   "metadata": {},
   "source": [
    "Now to help regulate the bias of LASSO we fit a Logistic Model using `glm` and the held-out set.  We fit a post-LASSO $w = 1$ (the \"Baseline\" or \"Unweighted\") model and a $w = w^{\\star}$ (\"Weighted\") model. We use the covariates saved in `models_list` from the main training loop for each model, implying that different covariates could be selected for the different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3fccf0-d0e6-4aab-89d2-68d5dd8aa6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“glm.fit: fitted probabilities numerically 0 or 1 occurred”\n",
      "Warning message:\n",
      "“glm.fit: fitted probabilities numerically 0 or 1 occurred”\n",
      "Warning message:\n",
      "“glm.fit: fitted probabilities numerically 0 or 1 occurred”\n"
     ]
    }
   ],
   "source": [
    "# Main developer: Caden\n",
    "# ------------------- #\n",
    "# --- Post-LASSO --- #\n",
    "# ------------------- #\n",
    "# get the columns of best f1 weight and append response\n",
    "post_lasso_df <- inference[, c(\"Attrition\", models_list[[best_w_f1]])]\n",
    "# compute weights according to best result\n",
    "weights_f1 <- ifelse(post_lasso_df$Attrition == 1, best_w_f1, 1)\n",
    "# use regular glm to train post-lasso model\n",
    "mod_post_lasso <- glm(Attrition ~ ., weights = weights_f1,\n",
    "                      data = post_lasso_df,family = \"binomial\")\n",
    "\n",
    "# repeat for fb weight\n",
    "post_lasso_df_b <- inference[, c(\"Attrition\", models_list[[best_w_fb]])]\n",
    "# compute weights according to best result\n",
    "weights_fb <- ifelse(post_lasso_df$Attrition == 1, best_w_fb, 1)\n",
    "# use regular glm to train post-lasso model\n",
    "fb_post_lasso <- glm(Attrition ~ ., weights = weights_fb,\n",
    "                      data = post_lasso_df, family = \"binomial\")\n",
    "\n",
    "# ------------------ #\n",
    "# ---- Baseline ---- #\n",
    "# ------------------ #\n",
    "# get the columns of the unweighted model from LASSO\n",
    "baseline_df <- inference[, c(\"Attrition\", models_list[[1]])]\n",
    "# use regular glm to train baseline post-lasso model\n",
    "baseline_post_lasso <- glm(Attrition ~ .,\n",
    "                           data = baseline_df,\n",
    "                           family = \"binomial\")\n",
    "# -------------------- #\n",
    "# -- Inference Set -- #\n",
    "# ------------------- #\n",
    "# baseline post-LASSO fitted values\n",
    "base_preds <- predict(baseline_post_lasso, newdata = inference, type = \"response\") > 0.50\n",
    "# weighted post-LASSO fitted values\n",
    "post_preds_f1 <- predict(mod_post_lasso, newdata = inference, type = \"response\") > 0.50\n",
    "post_preds_fb <- predict(fb_post_lasso, newdata = inference, type = \"response\") > 0.50\n",
    "# compute confusion matrix for baseline\n",
    "perf_PB <- compute_metrics(table(base_preds, inference$Attrition))\n",
    "# and for F1/FB models\n",
    "perf_PFB <- compute_metrics(table(post_preds_fb, inference$Attrition))\n",
    "perf_PF1 <- compute_metrics(table(post_preds_f1, inference$Attrition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfa9da-46c1-4b3f-b578-f5d52aa8acff",
   "metadata": {},
   "source": [
    "Further, we compute the out-of-sample performance for both post-LASSO `glm` fits, so we can assess whether the weighted model is predicting $\\text{Attrition} = 1$ better than the baseline model. In addition, we extract the training performance from the original LASSO fitting process for each model. We thus have three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a03d0d-524a-46b3-ab0e-c12935b150b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table\" style=\"font-size: 12px; margin-left: auto; margin-right: auto;\">\n",
       "<caption style=\"font-size: initial !important;\">Summary of Performance Metrics Across Models</caption>\n",
       " <thead>\n",
       "  <tr>\n",
       "   <th style=\"text-align:left;\">   </th>\n",
       "   <th style=\"text-align:left;\"> Accuracy </th>\n",
       "   <th style=\"text-align:left;\"> Precision </th>\n",
       "   <th style=\"text-align:left;\"> Sensitivity </th>\n",
       "   <th style=\"text-align:left;\"> Specificity </th>\n",
       "   <th style=\"text-align:left;\"> F<sub>1</sub> Score </th>\n",
       "   <th style=\"text-align:left;\"> F<sub>β</sub> Score </th>\n",
       "   <th style=\"text-align:left;\"> Variables Selected </th>\n",
       "  </tr>\n",
       " </thead>\n",
       "<tbody>\n",
       "  <tr grouplength=\"2\"><td colspan=\"8\" style=\"border-bottom: 1px solid;\"><strong>Fβ-Preferred Model</strong></td></tr>\n",
       "<tr>\n",
       "   <td style=\"text-align:left;padding-left: 2em;width: 2.5cm; \" indentlevel=\"1\"> Lasso </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.805 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.44 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.863 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.794 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.583 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.724 </td>\n",
       "   <td style=\"text-align:left;\"> 29 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:left;padding-left: 2em;width: 2.5cm; \" indentlevel=\"1\"> Post-Lasso </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.825 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.486 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.908 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.808 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.633 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.774 </td>\n",
       "   <td style=\"text-align:left;\">  </td>\n",
       "  </tr>\n",
       "  <tr grouplength=\"2\"><td colspan=\"8\" style=\"border-bottom: 1px solid;\"><strong>F1-Preferred Model</strong></td></tr>\n",
       "<tr>\n",
       "   <td style=\"text-align:left;padding-left: 2em;width: 2.5cm; \" indentlevel=\"1\"> Lasso </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.9 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.689 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.669 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.943 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.679 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.673 </td>\n",
       "   <td style=\"text-align:left;\"> 28 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:left;padding-left: 2em;width: 2.5cm; \" indentlevel=\"1\"> Post-Lasso </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.895 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.67 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.724 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.929 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.696 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.713 </td>\n",
       "   <td style=\"text-align:left;\">  </td>\n",
       "  </tr>\n",
       "  <tr grouplength=\"2\"><td colspan=\"8\" style=\"border-bottom: 1px solid;\"><strong>Unweighted Model</strong></td></tr>\n",
       "<tr>\n",
       "   <td style=\"text-align:left;padding-left: 2em;width: 2.5cm; \" indentlevel=\"1\"> Lasso </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.904 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.846 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.475 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.984 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.608 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.521 </td>\n",
       "   <td style=\"text-align:left;\"> 28 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:left;padding-left: 2em;width: 2.5cm; \" indentlevel=\"1\"> Post-Lasso </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.912 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.811 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.612 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.971 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.698 </td>\n",
       "   <td style=\"text-align:left;width: 2.5cm; \"> 0.644 </td>\n",
       "   <td style=\"text-align:left;\">  </td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main developer: Caden\n",
    "# ------------------- #\n",
    "# -- Clean Results -- #\n",
    "# ------------------- #\n",
    "\n",
    "# get the training performance for both weighted and unweighted\n",
    "lasso_FB <- t(results_rw[[best_w_fb]])[1:6]\n",
    "lasso_L <- t(results_rw[[best_w_f1]])[1:6]\n",
    "lasso_B <- t(results_rw[[1]])[1:6]\n",
    "# summarize results\n",
    "results = data.frame(\n",
    "  lasso_WTB = lasso_FB,\n",
    "  post_WTB  = round(unlist(perf_PFB), 3),\n",
    "  lasso_WT1 = lasso_L,\n",
    "  post_WT1  = round(unlist(perf_PF1), 3),\n",
    "  lasso_UW  = lasso_B,\n",
    "  post_UW   = round(unlist(perf_PB), 3)\n",
    ")\n",
    "# set column names (will become row names)\n",
    "colnames(results) <- rep(c(\"Lasso\", \"Post-Lasso\"), times = 3)\n",
    "# transpose the results matrix so it makes more sense when presented\n",
    "results <- t(results)\n",
    "# add the column selection metrics\n",
    "# select column names of both without \"Attrition\"\n",
    "S_fb <- models_list[[best_w_fb]]\n",
    "S_BL <- models_list[[1]]\n",
    "S_f1 <- models_list[[best_w_f1]]\n",
    "# append the number of selected variables for each model\n",
    "results <- cbind(results, K = c(length(S_fb), \"\",\n",
    "                                length(S_f1), \"\", \n",
    "                                length(S_BL), \"\"))\n",
    "# --------------------- #\n",
    "# -- Construct Table -- #\n",
    "# --------------------- #\n",
    "results_table <- kable(\n",
    "  results, \n",
    "  col.names = c(\n",
    "    \"Accuracy\", \"Precision\",\n",
    "    \"Sensitivity\", \"Specificity\",\n",
    "    \"F<sub>1</sub> Score\", \"F<sub>β</sub> Score\",\n",
    "    \"Variables Selected\"\n",
    "  ),\n",
    "  caption = \"Summary of Performance Metrics Across Models\",\n",
    "  escape = FALSE,  \n",
    "  format = \"html\"  \n",
    ") %>%\n",
    "  pack_rows(\"Fβ-Preferred Model\", 1, 2) %>%\n",
    "  pack_rows(\"F1-Preferred Model\", 3, 4) %>%\n",
    "  pack_rows(\"Unweighted Model\", 5, 6) %>%\n",
    "  kable_styling(\n",
    "    font_size = 12\n",
    "  ) %>% \n",
    "  column_spec(1:7, width = \"2.5cm\")\n",
    "\n",
    "# then hot potato table->html->png so we can show it in jupyter :(\n",
    "#save_kable(results_table, \"results_table.html\")\n",
    "# webshot::install_phantomjs()\n",
    "#webshot(\"results_table.html\", \"results_table.png\", zoom = 2, \n",
    "#        vheight = 400, vwidth = 750)\n",
    "\n",
    "results_table %>%\n",
    "  as.character() %>%\n",
    "  display_html()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8aff2-5ed1-4a72-afdd-39555c27a3ca",
   "metadata": {},
   "source": [
    "We now extract the $p$-values for the coefficients for both models and convert them to log scale. On each plot, we include vertical lines corresponding to the logarithm of $\\alpha \\in \\{0.01, 0.05, 0.10\\}$. For visual simplicity, we plot the numeric covariates on one plot and the categorical coefficients on another. The horizontal lines dictates the distance in log-$p$ values for coefficients between the weighted and unweighted models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcbbc1d1-f54c-44f5-b288-892d25f1c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main developer: Caden\n",
    "options(repr.plot.width = 22, repr.plot.height = 9)\n",
    "\n",
    "# --------------------- #\n",
    "# -- Combine Results -- #\n",
    "# --------------------- #\n",
    "# get coefficients from both post-LASSO models\n",
    "coef_table <- as.data.frame(summary(fb_post_lasso)$coefficients) %>% mutate(Coefficient = rownames(.))\n",
    "coef_baseline <- as.data.frame(summary(baseline_post_lasso)$coefficients) %>% mutate(Coefficient = rownames(.))\n",
    "# join them togeth by coefficients\n",
    "joined_coefs <- full_join(coef_table, coef_baseline, by = \"Coefficient\")\n",
    "# set p-value of NA coefficients to 1.0 (assuming not selected implies p = 1)\n",
    "joined_coefs$`Pr(>|z|).y`[which(is.na( joined_coefs$`Pr(>|z|).y`))] <- 1\n",
    "joined_coefs$`Pr(>|z|).x`[which(is.na( joined_coefs$`Pr(>|z|).x`))] <- 1\n",
    "# extract p-values for coefficients of weighted and unweighted models\n",
    "plotDF <- data.frame(Weighted = (joined_coefs$`Pr(>|z|).x`), \n",
    "                     Baseline = (joined_coefs$`Pr(>|z|).y`))\n",
    "# add the coefficient names as a column\n",
    "plotDF$variable <- joined_coefs$Coefficient\n",
    "# get names of numeric columns from original data frame\n",
    "numeric_cols <- colnames(X %>% select_if(is.numeric))\n",
    "\n",
    "# ----------------------- #\n",
    "# -- Numeric Coef Plot -- #\n",
    "# ----------------------- #\n",
    "# for the first plot, we do the p-values of numeric columns\n",
    "plotDF_N <- plotDF[plotDF$variable %in% numeric_cols, ]\n",
    "# we pivot the data to prepare it for plotting\n",
    "plotDF_long <- plotDF_N %>%\n",
    "  pivot_longer(cols = c(Weighted, Baseline), \n",
    "               names_to = \"Model\", \n",
    "               values_to = \"pval\")\n",
    "\n",
    "# create the plot of numeric coefficient p-values\n",
    "pN <- ggplot(plotDF_long, aes(x = pval, y = variable, color = Model)) +\n",
    "  # add the line first so that its not on top of the points\n",
    "  geom_line(aes(group = variable),\n",
    "            color = \"grey30\",\n",
    "            linewidth = 0.5) +\n",
    "  # add points for p-values\n",
    "  geom_point(size = 2) +\n",
    "  # convert to log-scale for readability\n",
    "  scale_x_log10() +\n",
    "  labs(x = \"P-value (log scale)\", y = \"Numeric Variable\", color = \"Model\") +\n",
    "  scale_color_manual(values = c(\n",
    "    \"Weighted\" = \"#004346\",\n",
    "    \"Baseline\" = \"#508991\"\n",
    "  )) +\n",
    "  geom_vline(xintercept = 0.01,\n",
    "             linetype = \"dashed\",\n",
    "             color = \"#75dddd\") +\n",
    "  geom_vline(xintercept = 0.05,\n",
    "             linetype = \"dashed\",\n",
    "             color = \"#09bc8a\") +\n",
    "  geom_vline(xintercept = 0.10,\n",
    "             linetype = \"dashed\",\n",
    "             color = \"#004346\") +\n",
    "  labs(title = \"Log P-Values for Numeric Variables, Post-LASSO Weighted vs Unweighted\", \n",
    "       subtitle = \"Vertical Lines are Significance {0.01, 0.05, 0.10} Respectively\") +\n",
    "  theme_bw() +\n",
    "  theme(\n",
    "    axis.text.y = element_text(size = 12),\n",
    "    axis.title.x = element_text(size = 14),\n",
    "    axis.title.y = element_text(size = 14),\n",
    "    plot.title = element_text(size = 15),\n",
    "    plot.subtitle = element_text(size = 12),\n",
    "    legend.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 12)\n",
    "  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada5713-5c65-470f-866f-7daeee22a5ed",
   "metadata": {},
   "source": [
    "For the second plot contanining categorical variables, there are many more $p$-values to be plotted than in the numeric model. This is due to the fact that there is a coefficient/dummy variable for each factor level beyond the baseline. Hence, we filter out those coefficients where both of the observed $p$-values are above the $10\\%$ significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709c060-d67d-4c98-8ee2-199e67be8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main developer: Caden\n",
    "# ------------------- #\n",
    "# -- Category Coef -- #\n",
    "# ------------------- #\n",
    "# for the second plot, we do the p-values of categorical columns\n",
    "plotDF_C <- plotDF[!plotDF$variable %in% numeric_cols, ]\n",
    "# however there are many more than numeric\n",
    "# so we select only those with at least one significant at 0.10\n",
    "plotDF_C <- plotDF_C[plotDF_C$Weighted < 0.10 | plotDF_C$Baseline < 0.10, ]\n",
    "# we pivot the data to prepare it for plotting\n",
    "plotDF_longC <- plotDF_C %>%\n",
    "  pivot_longer(cols = c(Weighted, Baseline), \n",
    "               names_to = \"Model\", \n",
    "               values_to = \"pval\")\n",
    "\n",
    "# create the plot of numeric coefficient p-values\n",
    "pC <- ggplot(plotDF_longC, aes(x = pval, y = variable, color = Model)) +\n",
    "  # add the line first so that its not on top of the points\n",
    "  geom_line(aes(group = variable),\n",
    "            color = \"grey30\",\n",
    "            linewidth = 0.5) +\n",
    "  # add points for p-values\n",
    "  geom_point(size = 2) +\n",
    "  # convert to log-scale for readability\n",
    "  scale_x_log10() +\n",
    "  labs(x = \"P-value (log scale)\", y = \"Categorical Variable\", color = \"Model\") +\n",
    "  scale_color_manual(values = c(\n",
    "    \"Weighted\" = \"#8c2f39\",\n",
    "    \"Baseline\" = \"#bf4342\"\n",
    "  )) +\n",
    "  geom_vline(xintercept = 0.01,\n",
    "             linetype = \"dashed\",\n",
    "             color = \"#723d46\") +\n",
    "  geom_vline(xintercept = 0.05,\n",
    "             linetype = \"dashed\",\n",
    "             color = \"#ff4d6d\") +\n",
    "  geom_vline(xintercept = 0.10,\n",
    "             linetype = \"dashed\",\n",
    "             color = \"#fcb9b2\") +\n",
    "  labs(title = \"Log P-Values for Categorical Variables, Post-LASSO Weighted vs Unweighted\", \n",
    "       subtitle = \"Vertical Lines are Significance {0.01, 0.05, 0.10} Respectively\") +\n",
    "  theme_bw() +\n",
    "  theme(\n",
    "    axis.text.y = element_text(size = 12),\n",
    "    axis.title.x = element_text(size = 14),\n",
    "    axis.title.y = element_text(size = 14),\n",
    "    plot.title = element_text(size = 15),\n",
    "    plot.subtitle = element_text(size = 12),\n",
    "    legend.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 12)\n",
    "  ) \n",
    "grid.arrange(pN, pC, ncol = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
